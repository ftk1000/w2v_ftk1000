# w2v_ftk1000
## essential files for building word2vec models using C code 
## py wrapper to play with the model


[w2v demo](https://github.com/Suji04/NormalizedNerd/blob/master/Introduction%20to%20NLP/Word2Vec.ipynb)<br>
[w2v demo on colab Malstm_quoraquestionpair.ipynb](https://colab.research.google.com/drive/1CXdKsk9mvHiuFMRowELp0NDP5__bsuVy#scrollTo=tfWFxxNGXqDr)<br>
  - downloads data from Kaggle compeition (Quota questions) and w2v embedding from s3.amazon (see below)
  - !wget -P /root/input/ -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
  - model malstm saved as model.h5<br>
<br>
[Word2Vec: Obtain word embeddings](https://chainer-colab-notebook.readthedocs.io/en/latest/notebook/official_example/word2vec.html)<br>
  - has some code + key ideas of the algos<br>
[MY COLAB: Copy of Malstm_quoraquestionpair.ipynb](https://colab.research.google.com/drive/1deTNTvPlzO0oRLCuyf377AviWXiS7xRm)<br>
[]()<br>
[]()<br>
[]()<br>
[как скачать данные из кагла в колаб](https://www.kaggle.com/c/made-thousand-facial-landmarks/discussion/143089)<br>
