# w2v_ftk1000

## Chris McCormick's Tutorials 
[2016: word2vec-tutorial-the-skip-gram-model (Part-1)](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)<br>
[2016: word2vec-tutorial-Negative Sampling (Part-2)](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)<br>[]()<br>
[]()<br>
[]()<br>

[]()<br>
[]()<br>

## TF on embedding
[This document details feature columns. Think of feature columns as the intermediaries between raw data and Estimators.](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/feature_columns.md)<br>
[http://projector.tensorflow.org/](http://projector.tensorflow.org/)<br>


## essential files for building word2vec models using C code 
[Run C code from py on COLAB: Lecture-6A-Fortran-and-C](https://colab.research.google.com/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-6A-Fortran-and-C.ipynb)<br>
## py wrapper to play with the model


[w2v demo](https://github.com/Suji04/NormalizedNerd/blob/master/Introduction%20to%20NLP/Word2Vec.ipynb)<br>
[w2v demo on colab Malstm_quoraquestionpair.ipynb](https://colab.research.google.com/drive/1CXdKsk9mvHiuFMRowELp0NDP5__bsuVy#scrollTo=tfWFxxNGXqDr)<br>
  - downloads data from Kaggle compeition (Quota questions) and w2v embedding from s3.amazon (see below)
  - !wget -P /root/input/ -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
  - model malstm saved as model.h5<br>
<br>
[Word2Vec: Obtain word embeddings](https://chainer-colab-notebook.readthedocs.io/en/latest/notebook/official_example/word2vec.html)<br>
  - has some code + key ideas of the algos<br>
[MY COLAB: Copy of Malstm_quoraquestionpair.ipynb](https://colab.research.google.com/drive/1deTNTvPlzO0oRLCuyf377AviWXiS7xRm)<br>
[]()<br>
[]()<br>
[]()<br>
[как скачать данные из кагла в колаб](https://www.kaggle.com/c/made-thousand-facial-landmarks/discussion/143089)<br>

# w2v in py
[2018: implement-your-own-word2vecskip-gram-model-in-python](https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/?ref=rp)<br>
[2018: python-word-embedding-using-word2vec](https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/)<br>

[Chainer: Word2Vec: Obtain word embeddings](https://chainer-colab-notebook.readthedocs.io/en/latest/notebook/official_example/word2vec.html)<br>
[]()<br>
[]()<br>
# w2v in C
[google's code](https://code.google.com/archive/p/word2vec/)<br>
[]()<br>

