# Mikolov Papers

[2013.06: Linguistic Regularities in Continuous Space Word Representations,
Tomas Mikolov, Wen-tau Yih, Geoffrey Zweig, Microsoft Research, Redmond, WA 98052](https://www.aclweb.org/anthology/N13-1090.pdf)<br>
  - we examine the vector-space word representations
that are implicitly learned by the input-layer
weights. We find that these representations
are surprisingly good at capturing syntactic
and semantic regularities in language, and
that each relationship is characterized by a
relation-specific vector offset. E.g., the male/female
relationship is automatically learned, and with
the induced vector representations, “King -
Man + Woman” ~ “Queen.”
  - We demonstrate that the word
vectors capture syntactic regularities by means
of syntactic analogy questions (provided with
this paper), and are able to correctly answer
almost 40% of the questions. We demonstrate
that the word vectors capture semantic regularities by using the vector offset method to
answer SemEval-2012 Task 2 questions.
  - In Section 2, we discuss related work; Section
3 describes the recurrent neural network language
model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed
vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.
  -  **CHALLENGE**: Early neural network language
models for distributed word representations
demonstrated outstanding performance in terms of
word-prediction, but also **the need for more computationally efficient models** . 
  - This has been addressed by subsequent work using **hierarchical prediction**
(Morin and Bengio, 2005; Mnih and Hinton, 2009;
Le et al., 2011; Mikolov et al., 2011b; Mikolov et
al., 2011a). 
  - The word representations we study are learned by a
recurrent neural network language model (Mikolov
et al., 2010)
  - To evaluate the vector offset method, we used
vectors generated by the RNN toolkit of Mikolov
(2012). Vectors of dimensionality 80, 320, and 640
were generated, along with a composite of several
systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast
News data as described in (Mikolov et al., 2011a),
and had an 82k vocabulary.
  - Table 2 shows results
for both RNNLM and LSA vectors on the syntactic
task. LSA was trained on the same data as the RNN.
We see that the RNN vectors capture significantly
more syntactic regularity than the LSA vectors, and
do remarkably well in an absolute sense, answering
more than one in three questions correctly. 

[2013.09: Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, Google Inc](https://arxiv.org/pdf/1301.3781.pdf)<br>
  - We propose two novel model architectures for computing continuous vector representations of words from very large data sets.
  - We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. 
  - The main goal of this paper is to introduce techniques that can be used for learning high-quality word
vectors from huge data sets with billions of words, and with millions of words in the vocabulary.
  - Many different types of models were proposed for estimating continuous representations of words,
including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).
In this paper, we focus on distributed representations of words learned by neural networks, as it was
previously shown that they perform significantly better than LSA for preserving linear regularities
among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.
  - The probabilistic feedforward neural network language model has been proposed in [1]. It consists
of input, projection, hidden and output layers. At the input layer, N previous words are encoded
using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a
projection layer P that has dimensionality N × D, using a shared projection matrix. As only N
inputs are active at any given time, composition of the projection layer is a relatively cheap operation.
  - Recurrent neural network based language model has been proposed to overcome certain limitations
of the feedforward NNLM, such as the need to specify the context length (the order of the model N),
and because theoretically RNNs can efficiently represent more complex patterns than the shallow
neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and
output layer. 
  - **New Log-linear Models:** we propose two new model architectures for learning *distributed representations of words* 
that try to minimize computational complexity. The main observation from the previous
section was that **most of the complexity is caused by the non-linear hidden layer** in the model. While
this is what makes neural networks so attractive, we decided to **explore simpler models** that might
not be able to represent the data as precisely as neural networks, but can possibly **be trained on much
more data efficiently**.
  - The new architectures directly follow those proposed in our earlier work [13, 14], where it was
found that neural network language model can be successfully trained in two steps: first, continuous
word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these
distributed representations of words. While there has been later substantial amount of work that
focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.
    - 3.1 **Continuous Bag-of-Words (CBOW) Model :** The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix);
thus, all words get projected into the same position (their vectors are averaged). We call this architecture a bag-of-words model as the order of words in the history does not influence the projection.
Furthermore, we also use words from the future; we have obtained the best performance on the task
introduced in the next section by building a log-linear classifier with four future and four history
words at the input, where the training criterion is to correctly classify the current (middle) word.
    - Training complexity is then                         $$Q = N × D + D × log2(V )$$
    - Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.


[]()<br>

[2014: Neural Word Embedding as Implicit Matrix Factorization, Levy, Goldberg](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf)<br>
    - We analyze skip-gram with negative-sampling (SGNS), a word embedding
method introduced by Mikolov et al., and show that it is implicitly factorizing
a word-context matrix, whose cells are the pointwise mutual information (PMI) of
the respective word and context pairs, shifted by a global constant. We find that
another embedding method, NCE, is implicitly factorizing a similar matrix, where
each cell is the (shifted) log conditional probability of a word given its context.
    - We show that using a sparse Shifted Positive PMI word-context matrix to represent
words improves results on two word similarity tasks and one of two analogy tasks.
When dense low-dimensional vectors are preferred, exact factorization with SVD
can achieve solutions that are at least as good as SGNS’s solutions for word similarity tasks. 
On analogy questions SGNS remains superior to SVD. We conjecture that this stems 
from the weighted nature of SGNS’s factorization



[]()<br>

[]()<br>

[]()<br>

